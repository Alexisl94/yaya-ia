# ü§ñ Int√©gration Anthropic Claude - yaya.ia

Documentation compl√®te de l'int√©gration de l'API Anthropic Claude pour les conversations IA.

## ‚ö†Ô∏è IMPORTANT - S√âCURIT√â

### Votre cl√© API

Votre cl√© API Anthropic a √©t√© configur√©e dans `.env.local`. **Cette cl√© est secr√®te et ne doit JAMAIS √™tre partag√©e ou commit√©e dans Git.**

**üî¥ ACTION REQUISE:**
1. Votre cl√© a √©t√© expos√©e dans ce chat
2. Apr√®s cette session, **r√©g√©n√©rez votre cl√© API** sur: https://console.anthropic.com/settings/keys
3. Mettez √† jour `.env.local` avec la nouvelle cl√©
4. V√©rifiez que `.env.local` est dans `.gitignore`

### Bonnes pratiques

```bash
# ‚úÖ TOUJOURS dans .gitignore
.env.local
.env*.local

# ‚ùå JAMAIS commiter
# .env.local avec cl√©s r√©elles
```

## üì¶ Architecture

```
lib/llm/
‚îú‚îÄ‚îÄ anthropic-client.ts          # Client API Anthropic
‚îú‚îÄ‚îÄ prompt-generator.ts          # G√©n√©rateur de prompts
‚îî‚îÄ‚îÄ README-ANTHROPIC.md          # Cette documentation

app/api/chat/
‚îî‚îÄ‚îÄ route.ts                     # Route API pour le chat
```

## üöÄ Fonctionnement

### 1. Flow complet

```
User tape message
      ‚Üì
MessageInput (client)
      ‚Üì
POST /api/chat
      ‚Üì
Validation auth + agent
      ‚Üì
Fetch historique (20 derniers messages)
      ‚Üì
Anthropic API (Claude)
      ‚Üì
Sauvegarde message en DB
      ‚Üì
Log usage (billing)
      ‚Üì
Retour au client
```

### 2. Code client (message-input.tsx)

```typescript
const response = await fetch('/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    message: "Bonjour !",
    agentId: "agent-id",
    conversationId: "conv-id"
  })
})

const data = await response.json()
// { success: true, message: {...}, usage: {...} }
```

### 3. API Route (/api/chat/route.ts)

**S√©curit√©:**
- ‚úÖ V√©rifie l'authentification
- ‚úÖ Valide que l'agent appartient √† l'utilisateur
- ‚úÖ Limite l'historique √† 20 messages (contr√¥le tokens)

**Fonctionnalit√©s:**
- Fetch agent + system prompt
- Fetch historique conversation
- Appel Claude API
- Sauvegarde messages en DB
- Log usage pour analytics/billing

### 4. Client Anthropic (anthropic-client.ts)

**Fonction principale: `sendMessage()`**

```typescript
const response = await sendMessage(
  systemPrompt: string,
  messages: Array<{role, content}>,
  options?: {
    model?: string,
    maxTokens?: number,
    temperature?: number
  }
)

// Retour:
// {
//   success: true,
//   content: "R√©ponse de Claude",
//   usage: {
//     input_tokens: 150,
//     output_tokens: 200
//   },
//   model: "claude-3-5-sonnet-20241022"
// }
```

**Fonction streaming: `streamMessage()`**

Pour les r√©ponses en temps r√©el (√† impl√©menter dans l'UI):

```typescript
for await (const chunk of streamMessage(systemPrompt, messages)) {
  if (chunk.type === 'content') {
    console.log(chunk.text) // Afficher token par token
  } else if (chunk.type === 'done') {
    console.log('Done!')
  }
}
```

## üéØ Mod√®les disponibles

### Claude 3.5 Sonnet (par d√©faut)
- **ID**: `claude-3-5-sonnet-20241022`
- **Contexte**: 200k tokens
- **Vitesse**: Rapide
- **Qualit√©**: Excellente
- **Co√ªt**: Mod√©r√©
- **Usage**: Recommand√© pour tous les agents

### Claude 3 Opus (premium)
- **ID**: `claude-3-opus-20240229`
- **Contexte**: 200k tokens
- **Vitesse**: Plus lent
- **Qualit√©**: Maximale
- **Co√ªt**: √âlev√©
- **Usage**: T√¢ches complexes uniquement

### Claude 3 Haiku (√©conomique)
- **ID**: `claude-3-haiku-20240307`
- **Contexte**: 200k tokens
- **Vitesse**: Tr√®s rapide
- **Qualit√©**: Bonne
- **Co√ªt**: Faible
- **Usage**: T√¢ches simples, prototypes

## üí∞ Co√ªts Anthropic (Nov 2024)

### Claude 3.5 Sonnet
- Input: $3 / million tokens
- Output: $15 / million tokens

### Exemple de calcul
```
Conversation typique:
- System prompt: ~1000 tokens
- Historique (20 msg): ~2000 tokens
- Message user: ~50 tokens
- R√©ponse Claude: ~300 tokens

Total input: 3050 tokens = $0.009
Total output: 300 tokens = $0.0045
CO√õT TOTAL: ~$0.014 par message
```

### Optimisations

**1. Limiter l'historique**
```typescript
// Dans route.ts
.limit(20) // ‚Üê Ajuster selon le besoin
```

**2. System prompt concis**
```typescript
// Dans prompt-generator.ts
// √âviter les r√©p√©titions
// Aller √† l'essentiel
```

**3. Temp√©rature plus basse**
```typescript
temperature: 0.7 // Au lieu de 1 pour r√©ponses plus courtes
```

## üîß Configuration

### Variables d'environnement

```bash
# .env.local
ANTHROPIC_API_KEY=sk-ant-api03-...

# Optionnel - limites de s√©curit√©
MAX_TOKENS_PER_REQUEST=4096
MAX_MESSAGES_HISTORY=20
```

### Param√®tres par agent

Dans la DB, chaque agent peut avoir:

```typescript
{
  model: 'claude', // ou 'gpt' (fallback)
  temperature: 1,  // 0-2 (cr√©ativit√©)
  max_tokens: 4096 // Limite de r√©ponse
}
```

## üìä Monitoring

### Usage logs

Chaque message est logu√© dans `usage_logs`:

```typescript
{
  user_id: "...",
  agent_id: "...",
  conversation_id: "...",
  event_type: "message",
  model_used: "claude-3-5-sonnet-20241022",
  tokens_used: 350,
  metadata: {
    input_tokens: 3050,
    output_tokens: 300,
    latency_ms: 1234
  }
}
```

### Dashboard analytics (√† cr√©er)

Requ√™tes SQL utiles:

```sql
-- Tokens par utilisateur (dernier mois)
SELECT
  user_id,
  SUM(tokens_used) as total_tokens,
  COUNT(*) as messages_count
FROM usage_logs
WHERE created_at > NOW() - INTERVAL '30 days'
GROUP BY user_id;

-- Co√ªt estim√© par agent
SELECT
  agent_id,
  SUM(
    (metadata->>'input_tokens')::int * 0.000003 +
    (metadata->>'output_tokens')::int * 0.000015
  ) as estimated_cost_usd
FROM usage_logs
WHERE model_used LIKE 'claude-3-5-sonnet%'
GROUP BY agent_id;
```

## üß™ Tests

### Test l'API directement

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "Bonjour, qui es-tu ?",
    "agentId": "agent-1",
    "conversationId": "conv-1"
  }'
```

### Test avec diff√©rents mod√®les

Modifier temporairement dans `route.ts`:

```typescript
const response = await sendMessage(
  agent.system_prompt,
  conversationHistory,
  {
    model: 'claude-3-opus-20240229', // Tester Opus
    // model: 'claude-3-haiku-20240307', // Tester Haiku
  }
)
```

## üöÄ Prochaines √©tapes

### 1. Streaming (temps r√©el)

Cr√©er `/api/chat/stream/route.ts`:

```typescript
export async function POST(request: NextRequest) {
  const encoder = new TextEncoder()

  const stream = new ReadableStream({
    async start(controller) {
      for await (const chunk of streamMessage(systemPrompt, messages)) {
        if (chunk.type === 'content') {
          controller.enqueue(
            encoder.encode(`data: ${JSON.stringify(chunk)}\n\n`)
          )
        }
      }
      controller.close()
    }
  })

  return new Response(stream, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
    }
  })
}
```

### 2. Multi-modal (images)

Claude supporte les images:

```typescript
messages: [{
  role: 'user',
  content: [
    { type: 'text', text: 'Que vois-tu sur cette image ?' },
    {
      type: 'image',
      source: {
        type: 'base64',
        media_type: 'image/jpeg',
        data: base64Image
      }
    }
  ]
}]
```

### 3. Rate limiting

Ajouter middleware:

```typescript
// middleware.ts
import { Ratelimit } from '@upstash/ratelimit'

const ratelimit = new Ratelimit({
  redis: redis,
  limiter: Ratelimit.slidingWindow(10, '1 m'), // 10 req/min
})

export async function middleware(request: NextRequest) {
  if (request.nextUrl.pathname === '/api/chat') {
    const { success } = await ratelimit.limit(userId)
    if (!success) {
      return new Response('Rate limit exceeded', { status: 429 })
    }
  }
}
```

### 4. Prompt caching (√©conomies)

Claude 3.5 Sonnet supporte le prompt caching:

```typescript
const response = await anthropic.messages.create({
  model: 'claude-3-5-sonnet-20241022',
  system: [
    {
      type: 'text',
      text: systemPrompt,
      cache_control: { type: 'ephemeral' } // ‚Üê Cache 5 min
    }
  ],
  // ...
})

// √âconomies: 90% sur les tokens mis en cache
```

## üêõ Troubleshooting

### Erreur: "API key not found"

```bash
# V√©rifier que la cl√© est bien dans .env.local
cat .env.local | grep ANTHROPIC

# Red√©marrer le serveur
npm run dev
```

### Erreur: "Invalid API key"

1. V√©rifier sur https://console.anthropic.com/settings/keys
2. R√©g√©n√©rer si n√©cessaire
3. Mettre √† jour `.env.local`

### Erreur: "Rate limit exceeded"

Anthropic a des limites par d√©faut:
- Tier 1: 50 requests/min
- Tier 2: 1000 requests/min

Solution: Upgrade sur https://console.anthropic.com/settings/limits

### R√©ponses lentes

Optimisations:
1. R√©duire l'historique (< 20 messages)
2. System prompt plus court
3. Utiliser Haiku pour prototypes
4. Impl√©menter le streaming

## üìö Ressources

- [Anthropic API Docs](https://docs.anthropic.com/)
- [Mod√®les Claude](https://docs.anthropic.com/en/docs/models-overview)
- [Pricing](https://www.anthropic.com/pricing)
- [Prompt Engineering](https://docs.anthropic.com/en/docs/prompt-engineering)
- [Rate Limits](https://docs.anthropic.com/en/api/rate-limits)

---

**Cr√©√© avec ‚ù§Ô∏è pour yaya.ia**
